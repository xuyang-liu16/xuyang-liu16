ðŸŒˆ I am a second-year Master's student at [Sichuan University](https://en.scu.edu.cn/), supervised by Prof. [Honggang Chen](https://sites.google.com/view/honggangchen/). I am working as a research intern at [Ant Security Lab](https://securitylab.antgroup.com/home), part of Ant Group, focusing on GUI Agent. <!--Previously, I had the honor of visiting the [VIP Lab](https://zhengfenglab.com/) at [SUSTech](https://www.sustech.edu.cn/en/), supervised by Prof. [Feng Zheng](https://faculty.sustech.edu.cn/?tagid=fengzheng&go=1&iscss=1&snapid=1&lang=en).--> Previously, I completed an 8-month internship at [Taobao & Tmall Group](https://talent.taotian.com/), part of Alibaba Group, working on Efficient MLLM. I've also spent half a year visiting [MiLAB](https://milab.westlake.edu.cn/) at Westlake University, supervised by Prof. [Donglin Wang](https://en.westlake.edu.cn/faculty/donglin-wang.html). I am very glad to be supervised and collaborated with Dr. [Siteng Huang](https://kyonhuang.top/) from DAMO Academy and Asst. Prof. [Linfeng Zhang](http://www.zhanglinfeng.tech/) from SJTU.

<!-- ðŸ“Œ My research interests span **Efficient Multi-modal Large Language Models**, including:

* **Discrimination**: [visual grounding](https://github.com/linhuixiao/Awesome-Visual-Grounding) and [referring video object segmentation](https://github.com/gaomingqi/Awesome-Video-Object-Segmentation).
* **Adaptation**: [parameter-efficient transfer learning](https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning) and [model compression](https://github.com/MingSun-Tse/Efficient-Deep-Learning).  
* **Reconstruction**: [super-resolution](https://github.com/ChaofWang/Awesome-Super-Resolution) and [image quality assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment).
* **Generation**: [text-to-image generation](https://github.com/AlonzoLeeeooo/awesome-text-to-image-studies) and [text-to-video generation](https://github.com/soraw-ai/Awesome-Text-to-Video-Generation). -->

ðŸ“Œ My research interests span **Efficient Vision-Language Models**, including:
* **Efficient Inference**: [VidCom<sup>2</sup>](https://arxiv.org/abs/2505.14454), [GlobalCom<sup>2</sup>](https://arxiv.org/abs/2501.05179), [FiCoCo](https://arxiv.org/abs/2411.17686), [ToCa](https://arxiv.org/abs/2410.05317), [Dense-Tuning](https://arxiv.org/abs/2405.14700)
* **Efficient Training**: [M2IST](https://arxiv.org/abs/2407.01131), [V-PETL Bench](https://openreview.net/forum?id=yS1dUkQFnu), [DARA](https://arxiv.org/abs/2405.06217), [Dense-Tuning](https://arxiv.org/abs/2405.14700), [AutoGnothi](https://arxiv.org/abs/2410.21815)

ðŸ“¢ Recently, I am mainly focusing on **[Token-level Model Compression](https://arxiv.org/abs/2505.19147)**, and applying it to efficient **high-resolution understanding and long video understanding**. Feel free to reach out to me via Email `liuxuyang@stu.scu.edu.cn`, if you are interested in collaborating with me.

  
![Github stats](https://github-readme-stats.vercel.app/api?username=xuyang-liu16&theme=github_dark_dimmed&show_icons=true&count_private=true&layout=compact)

